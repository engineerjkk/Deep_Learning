# YOLOv3
YOLO는 You Only Look Once의 약자로 영상에서 객체의 위치와 크기를 구하는 알고리즘입니다.
이거에 대한 논문은 [YOLOv3](https://arxiv.org/abs/1804.02767) 에 들어가셔서 자세한 내용을 볼 수 있습니다. 

상황상 깊이까지는 들어가지 않겠지만 네트워크의 입력과 출력은 잘 아셔야합니다.

 1. 기본적인 입력은 416x416 컬러영상이 되어있는데 0~1로 Normalize되어있습니다. 컬러채널은 BGR이 아니라 RGB입니다.  

네트워크 구조에서 출력을 총 세번해주는데   
![image](https://user-images.githubusercontent.com/76835313/127802863-b286aed3-d059-41e1-849f-fb4290b3a831.png)

1)82번째 레이어에서 한번 출력해 줍니다. (507x85)형태의 matrix  
> 여기에서는 가로 13개 세로 13개로나누로 세개의 박스가 있고 각가게 대해서 4개의 박스에대한좌표 컨피던스, 어떤클래스냐를 다 저장합니다.)  
> 그래서 이 박스에서는 507 x 85 개의 정보를 나타내줍니다.
2)94번째 레이어에서 두번째 출력해 줍니다. (2028x85)형태의 matrix  
> 2028x85
3)106번째 레이어에서 세번째 출력해줍니다.(8112x85)형태의 metrix  
> 8112x85
그리고 이 세장의 이미지를 활용하고 조합해 내가 찾고자하는 객체를 찾는 것입니다. 여기선 후작업이 필요한 것이죠.  

여기서 일단 세장을 겹쳐놓고 confidence가 가장 높은 값을 선별해서 사용하면 됩니다.  
![image](https://user-images.githubusercontent.com/76835313/127802960-6e0c11d6-a0e2-4a5a-a083-40d2d43db412.png)
85가 뜻하는것은

앞의 4는 객체 중심 위치 좌표인 x,y와 width와 height를 출력, 

5번째는 Objectness Socre라고하는 객체를 잘 찾았냐 못찾았냐이지만 confidence와 비슷한 값이라고 보면될것같습니다.

나머지 80은 classscores입니다.

이후 OpenCV를 통해 후가공해 바운딩 박스 처리를 해줍니다.

바운딩박스들을 다 겹쳐놓고 그 중에서 가장 좋은박스를 선별하는 과정을 거친다. 비최대억제가 된다.   

outs의 ndarray를 다 체크하는데 클래스의 argmax를 뽑아 class_id를 뽑고 이값의 확률을 confidence에 넣는다.

그리고 이러한 박스들을 boxes라는 리스트에다가 일단은 쟁여놓는다.

그리고 NMSBoxes에서 nmsThreshold를 약 40 주면 박스에서 40%겹치는것들 중에서 confidence가 가장 큰것 하나만 넣어준다.   
