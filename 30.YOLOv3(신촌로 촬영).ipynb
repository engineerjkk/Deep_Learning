{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\rkdwn\\anaconda3\\lib\\site-packages (4.5.1.48)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\rkdwn\\anaconda3\\lib\\site-packages (from opencv-python) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "욜로 모델과 설정 파일을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'yolo_v3/yolov3.weights'\n",
    "config = 'yolo_v3/yolov3.cfg'\n",
    "class_labels = 'yolo_v3/coco.names'\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 이미지 파일을 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('sinchonro.mp4')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 생성 \n",
    "readNet 함수사용하며, net으로 네트워크 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래스 이름\n",
    "텍스트파일이므로 각 라인별로 스트링으로 뽑아서 리스트로 만들어 classes에 저장을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스에 80개가 들어가는데 임의로 색깔을 랜덤으로 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 레이어 이름을 받아옵니다.  \n",
    "output_layers = ['yolo_82', 'yolo_94', 'yolo_106']  이렇게 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fourcc는 정수값을 리턴해 줍니다.  \n",
    "저장할 동영상의 코덱을 DIVX로 설정해 주며, output.avi로 저장합니다. 현재 이 부분은 잘 안되고있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = round(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')  \n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, (w, h))\n",
    "#output.avi이름으로 저장하고, 압축방식은 fourcc DIVX로 저장합니다. 마지막은 default로했기때문에 컬러로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cap.isOpened():\n",
    "    print(\"Video open failed!\")\n",
    "# 실행\n",
    "else :    \n",
    "    #이미지 프레임 사이즈값을 받습니다.\n",
    "    w = round(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    #전체 프레임 수값을 받습니다.\n",
    "    frame_cnt = round(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #초당 프레임 값을받습니다.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    #한 프레임당 걸리는 시간을 받습니다.\n",
    "    delay = int(1000 / fps)\n",
    "    #영상을 저장할 코덱정보를 저장합니다.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'DIVX') # *'DIVX' == 'D', 'I', 'V', 'X'\n",
    "    #영상을 저장할 이름을 지정합니다.\n",
    "    outputVideo = cv2.VideoWriter('junekoo.avi', fourcc, fps, (w, h))\n",
    "    \n",
    "    \n",
    "    #전체 프레임까지 영상 루프를 돌립니다.\n",
    "    for i in range(frame_cnt): \n",
    "        ret, frame = cap.read()\n",
    "        #매 프레임마다 얼굴 검출\n",
    "        if not ret:\n",
    "            break\n",
    "        #각 프레임번째 수를 화면에 함께 출력해 알려줄 것입니다.    \n",
    "        label2='%d'%(i)\n",
    "        \n",
    "        #원본 이미지가 너무 크므로 가로세로 절반씩 리사이즈 해줬습니다. 보기편하게 하기 위해서 입니다.\n",
    "        h, w = frame.shape[:2]\n",
    "        frame = cv2.resize(frame, (w//2, h//2))\n",
    "        \n",
    "        # 블롭 생성 & 추론 #RGB하니까 True로 !\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255., (320, 320), swapRB=True)\n",
    "\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "        #forward해주고 문자열 리스트를 넣어줍니다.  outs를 리턴\n",
    "\n",
    "        # outs는 3개의 ndarray 리스트.\n",
    "        # outs[0].shape=(507, 85), 13*13*3=507\n",
    "        # outs[1].shape=(2028, 85), 26*26*3=2028\n",
    "        # outs[2].shape=(8112, 85), 52*52*3=8112\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        \n",
    "        #outs에 있는 ndarray를 다 체크하는데 하나의 행은 85개짜리의 elements, 좌표(4)+objectscore(1)+확률(80)\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                # detection: 4(bounding box) + 1(objectness_score) + 80(class confidence)\n",
    "                scores = detection[5:]\n",
    "                #80개에서 5번째 최대값위치의 id값을 뽑아와서 그 위치에서의 confidence를 체크합니다.\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > confThreshold:\n",
    "                    #확률이 0.5이상이면 바운딩박스를 취합니다.\n",
    "                    # 바운딩 박스 중심 좌표 & 박스 크기\n",
    "                    #0~1사이로 노말라이제이션 되어있어서 w,h를 곱해줘야합니다.\n",
    "                    cx = int(detection[0] * w)\n",
    "                    cy = int(detection[1] * h)\n",
    "                    bw = int(detection[2] * w)\n",
    "                    bh = int(detection[3] * h)\n",
    "\n",
    "                    # center 좌표로 바운딩 박스 좌상단 좌표추출\n",
    "                    sx = int(cx - bw / 2)\n",
    "                    sy = int(cy - bh / 2)\n",
    "\n",
    "                    boxes.append([sx, sy, bw, bh])\n",
    "                    #좌표를 묶어서 boxes에 쟁여놓습니다.\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(int(class_id))\n",
    "                    #일단 confidences와 class에 쟁여놓습니다..\n",
    "                    #겹치는게 생기는데 그중 좋은걸 쓸거다 비최대억제에서\n",
    "\n",
    "        # 비최대 억제\n",
    "        #boxes,confidence에서 스레시홀드가 0.4이상으로 되는걸 골라서(박스가 40%정도 겹친다). 그중에서 confidence가 0.5이상인것들 중에서 가장큰 confidence를 indices에다가 넣어준다.\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "        #몇번째 박스를 쓸거인지가 저장된다.\n",
    "\n",
    "        for i in indices:#N행1열\n",
    "            i = i[0]\n",
    "            sx, sy, bw, bh = boxes[i] #i번째 박스정보를 다시 가져옵니다.\n",
    "            label = f'{classes[class_ids[i]]}: {confidences[i]:.2}'\n",
    "            #문자열과 confidence 가져와서 아래에 출력합니다.\n",
    "            color = colors[class_ids[i]]\n",
    "            cv2.rectangle(frame, (sx, sy, bw, bh), color, 2)\n",
    "            cv2.putText(frame, label, (sx, sy - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        t, _ = net.getPerfProfile()\n",
    "        #인퍼런스시킬때 시간을 알아서 측정해줍니다. 실행하는데 시간이 얼마큼 걸렸다를 화면 좌측상단에 표시해 줍니다.\n",
    "\n",
    "        label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        frame=cv2.putText(frame,label2 , (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255))\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('frame', frame)\n",
    "        outputVideo.write(frame)\n",
    "        cv2.waitKey(delay)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "            \n",
    "outputVideo.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
