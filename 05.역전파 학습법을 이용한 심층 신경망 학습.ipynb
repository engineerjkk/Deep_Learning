{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치 미분을 이용한 심층 신경망 학습  \n",
    "### Back Propagation을 이용해서 Gradient Descent을 실제로 구현해 보겠습니다.  \n",
    "\n",
    "역전파에서 중요한 것은 체인룰과 다이나믹 프로그램입니다. \n",
    "다이나믹 프로그램 : 나중에 쓸 계산값을 미리 저장해 놓고 필요할 때 불러다 쓰는 것입니다.\n",
    "체인 룰 : 미분을 계산할 때 한번에 미분을 계산하지 않고 그 중간중간 미분들을 이용해서 각 미분값을 곱해서 미분값을 구하는 방식입니다.  \n",
    "\n",
    "두가지의 콜라보로 인해서 역전파 학습이 가능하다. 그러기 위해서는 다이나믹 프로그램을 위한 메모리를 잡아야 하기때문에 클래스를 만들어서 필요한 메모리를 저장할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid 구현  \n",
    "sigmoid를 call 할때마다 필요한 값들을 미리 저장해 놓고 bp를 할때 grad같은경우는 다음레이어와 입력 레이어 사이의 미분을 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self): #마지막 출력을 미리 1로 초기화해서 저장해 놓습니다.\n",
    "        self.last_o=1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.last_o=1/(1.0+np.exp(-x)) #sigmoid 함수의 정의\n",
    "        return self.last_o\n",
    "    def grad(self): #sigmoid(x)(1-sigmoid(x))\n",
    "        return self.last_o*(1-self.last_o)\n",
    "    #마지막에 저장된것을 받고 현재 gradient를 곱해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error 구현  \n",
    "한번 콜할때 필요한걸 저장하고 grad을 계산하면 다음레이어로부터 입력으로까지의 미분을 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        #gradient\n",
    "        self.dh=1 #gradient를 저장할 수 있는 공간을 만듭니다. \n",
    "        self.last_diff=1\n",
    "    def __call__(self, h, y): # 1/2 * mean(h-y)^2->입력이 미분되면 제곱이 내려와 상수가 상쇄됩니다.\n",
    "        self.last_diff=h-y\n",
    "        return 1/2*np.mean(np.square(h-y))\n",
    "\n",
    "    def grad(self): #h-y가 되겠죠.\n",
    "        return self.last_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴런 구현  \n",
    "뉴런은 Fully Connected Layer 입니다.  \n",
    "뉴런도 마찬가지로 call을 할때 필요한 것들을 미리 저장하고 grad를 이용하면 다음 레이어의 출력 입력 사이의 미분을 구하는데 grad를 이용합니다.  \n",
    "\n",
    "grad_b : 지금까지의 미분을 다곱해서 입력으로 받아서 b를 어떻게 업데이트 할것인지, loss function을 b로 미분한 결과를 리턴해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, W, b, a_obj): #모델의 파라미터를 저장합니다.\n",
    "        # Model parameters\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.a=a_obj() #sigmoid class를 받아줘야 합니다.\n",
    "        \n",
    "        #gradient\n",
    "        self.dW=np.zeros_like(self.W)\n",
    "        self.dW=np.zeros_like(self.b)\n",
    "        self.dh=np.zeros_like(_t(self.W))\n",
    "        \n",
    "        self.last_x=np.zeros((self.W.shape[0])) #마지막 x를 저장해줍니다.W를 미분했을때 이전 입력이 있어야 미분할 수 있으니 마지막 x를 미리 저장해 둡니다. w gradient를 위해서 입니다.\n",
    "        self.last_h=np.zeros((self.W.shape[1])) #마지막으로 나가는 출력도 저장을 해 줍니다.\n",
    "    def __call__(self, x):#콜을 했을 때 마지막으로 들어온 입력을 기억해 줍니다.\n",
    "        self.last_x=x\n",
    "        self.last_h=_m(_t(self.W),x)+self.b\n",
    "        return self.a(self.last_h)\n",
    "\n",
    "    def grad(self): #hn-1\n",
    "        return self.W * self.a.grad() #W만 살아남고 activation을 곱해줍니다.\n",
    "                    \n",
    "    def grad_W(self, dh): #weight로 미분을 할 경우. x만 남습니다. dy/dw=x\n",
    "        grad=np.ones_like(self.W)\n",
    "        grad_a=self.a.grad()\n",
    "        for j in range(grad.shape[1]):#1은 출력쪽 디멘션\n",
    "            grad[:,j]=dh[j] * grad_a[j]*self.last_x\n",
    "            # y=w^Tx+b\n",
    "        return grad #w로 미분한 gradient가 출력됩니다.\n",
    "    def grad_b(self, dh): #bias입니다. y=Wx+b에서 b로 미분하면 1이 되죠.\n",
    "        return dh * self.a.grad() *1 #b로 미분하면 1이기때문에 마지막에 1을 곱합니다. 없어도 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층신경망 구현\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "\n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(input, num_neuron)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index in range(hidden_depth):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Neuron(W, b, activation))\n",
    "\n",
    "        # Output Layer\n",
    "        W, b = init_var(num_neuron, output)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_gradient(self, loss_obj):#전체 미분을 통합해 보겠습니다.\n",
    "        loss_obj.dh=loss_obj.grad() #dh는 전부 gradient 입니다.\n",
    "        self.sequence.append(loss_obj)\n",
    "        #back-propagation loop를 구현해 보겠습니다.\n",
    "        for i in range(len(self.sequence)-1,0,-1):\n",
    "            l1=self.sequence[i]\n",
    "            l0=self.sequence[i-1]\n",
    "            \n",
    "            l0.dh=_m(l0.grad(),l1.dh)\n",
    "            l0.dw=l0.grad_W(l1.dh) \n",
    "            l0.db=l0.grad_b(l1.dh) #loss function을 b로도 미분해서 저장해 줍니다.\n",
    "        self.sequence.remove(loss_obj)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)\n",
    "list(range(6-1,0,-1))  \n",
    "\n",
    "5 4 3 2 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강 학습법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = loss_obj(network(x), y)  # Forward inference\n",
    "    network.calc_gradient(loss_obj)  # Back-propagation\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동작 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test loss 0.1114121335078407\n",
      "Epoch 1: Test loss 0.111272899385605\n",
      "Epoch 2: Test loss 0.11113384158676853\n",
      "Epoch 3: Test loss 0.1109949601948446\n",
      "Epoch 4: Test loss 0.11085625529147256\n",
      "Epoch 5: Test loss 0.11071772695642289\n",
      "Epoch 6: Test loss 0.11057937526760278\n",
      "Epoch 7: Test loss 0.1104412003010612\n",
      "Epoch 8: Test loss 0.11030320213099433\n",
      "Epoch 9: Test loss 0.11016538082975127\n",
      "Epoch 10: Test loss 0.11002773646783898\n",
      "Epoch 11: Test loss 0.10989026911392807\n",
      "Epoch 12: Test loss 0.10975297883485816\n",
      "Epoch 13: Test loss 0.1096158656956435\n",
      "Epoch 14: Test loss 0.10947892975947852\n",
      "Epoch 15: Test loss 0.10934217108774334\n",
      "Epoch 16: Test loss 0.10920558974000932\n",
      "Epoch 17: Test loss 0.10906918577404512\n",
      "Epoch 18: Test loss 0.10893295924582169\n",
      "Epoch 19: Test loss 0.10879691020951857\n",
      "Epoch 20: Test loss 0.10866103871752947\n",
      "Epoch 21: Test loss 0.10852534482046779\n",
      "Epoch 22: Test loss 0.10838982856717264\n",
      "Epoch 23: Test loss 0.10825449000471472\n",
      "Epoch 24: Test loss 0.10811932917840184\n",
      "Epoch 25: Test loss 0.1079843461317852\n",
      "Epoch 26: Test loss 0.10784954090666482\n",
      "Epoch 27: Test loss 0.10771491354309594\n",
      "Epoch 28: Test loss 0.10758046407939457\n",
      "Epoch 29: Test loss 0.10744619255214369\n",
      "Epoch 30: Test loss 0.10731209899619909\n",
      "Epoch 31: Test loss 0.10717818344469554\n",
      "Epoch 32: Test loss 0.10704444592905267\n",
      "Epoch 33: Test loss 0.10691088647898109\n",
      "Epoch 34: Test loss 0.1067775051224886\n",
      "Epoch 35: Test loss 0.10664430188588594\n",
      "Epoch 36: Test loss 0.10651127679379348\n",
      "Epoch 37: Test loss 0.10637842986914674\n",
      "Epoch 38: Test loss 0.10624576113320294\n",
      "Epoch 39: Test loss 0.10611327060554723\n",
      "Epoch 40: Test loss 0.10598095830409855\n",
      "Epoch 41: Test loss 0.10584882424511627\n",
      "Epoch 42: Test loss 0.10571686844320613\n",
      "Epoch 43: Test loss 0.10558509091132662\n",
      "Epoch 44: Test loss 0.10545349166079532\n",
      "Epoch 45: Test loss 0.10532207070129504\n",
      "Epoch 46: Test loss 0.10519082804088031\n",
      "Epoch 47: Test loss 0.10505976368598367\n",
      "Epoch 48: Test loss 0.10492887764142192\n",
      "Epoch 49: Test loss 0.10479816991040254\n",
      "Epoch 50: Test loss 0.10466764049453024\n",
      "Epoch 51: Test loss 0.10453728939381318\n",
      "Epoch 52: Test loss 0.10440711660666928\n",
      "Epoch 53: Test loss 0.10427712212993295\n",
      "Epoch 54: Test loss 0.10414730595886137\n",
      "Epoch 55: Test loss 0.1040176680871409\n",
      "Epoch 56: Test loss 0.1038882085068937\n",
      "Epoch 57: Test loss 0.10375892720868411\n",
      "Epoch 58: Test loss 0.10362982418152517\n",
      "Epoch 59: Test loss 0.10350089941288511\n",
      "Epoch 60: Test loss 0.10337215288869395\n",
      "Epoch 61: Test loss 0.10324358459335\n",
      "Epoch 62: Test loss 0.10311519450972641\n",
      "Epoch 63: Test loss 0.10298698261917763\n",
      "Epoch 64: Test loss 0.1028589489015463\n",
      "Epoch 65: Test loss 0.10273109333516944\n",
      "Epoch 66: Test loss 0.10260341589688517\n",
      "Epoch 67: Test loss 0.10247591656203954\n",
      "Epoch 68: Test loss 0.10234859530449282\n",
      "Epoch 69: Test loss 0.10222145209662631\n",
      "Epoch 70: Test loss 0.10209448690934897\n",
      "Epoch 71: Test loss 0.10196769971210388\n",
      "Epoch 72: Test loss 0.10184109047287515\n",
      "Epoch 73: Test loss 0.10171465915819448\n",
      "Epoch 74: Test loss 0.10158840573314759\n",
      "Epoch 75: Test loss 0.10146233016138127\n",
      "Epoch 76: Test loss 0.10133643240510985\n",
      "Epoch 77: Test loss 0.10121071242512186\n",
      "Epoch 78: Test loss 0.10108517018078683\n",
      "Epoch 79: Test loss 0.1009598056300618\n",
      "Epoch 80: Test loss 0.10083461872949832\n",
      "Epoch 81: Test loss 0.1007096094342489\n",
      "Epoch 82: Test loss 0.10058477769807381\n",
      "Epoch 83: Test loss 0.10046012347334773\n",
      "Epoch 84: Test loss 0.1003356467110667\n",
      "Epoch 85: Test loss 0.10021134736085449\n",
      "Epoch 86: Test loss 0.10008722537096967\n",
      "Epoch 87: Test loss 0.09996328068831206\n",
      "Epoch 88: Test loss 0.09983951325842974\n",
      "Epoch 89: Test loss 0.09971592302552544\n",
      "Epoch 90: Test loss 0.0995925099324637\n",
      "Epoch 91: Test loss 0.09946927392077723\n",
      "Epoch 92: Test loss 0.0993462149306739\n",
      "Epoch 93: Test loss 0.09922333290104332\n",
      "Epoch 94: Test loss 0.09910062776946382\n",
      "Epoch 95: Test loss 0.09897809947220884\n",
      "Epoch 96: Test loss 0.09885574794425414\n",
      "Epoch 97: Test loss 0.09873357311928405\n",
      "Epoch 98: Test loss 0.09861157492969859\n",
      "Epoch 99: Test loss 0.09848975330662008\n",
      "0.11202359199523926 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "t = time.time()\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)\n",
    "loss_obj = MeanSquaredError()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강 학습법에서 'Forward inference'가 먼저 한번 계산이 됩니다.\n",
    "2. 한번씩 콜이 일어날때 last_diff이 불려지고 저장이 됩니다.\n",
    "3. 뉴런에서도 콜이 되면서 last_x last_h 가 저장이 됩니다.\n",
    "4. sigmoid도 한번씩 콜이 되면서 last_o가 저장이 됩니다.\n",
    "5. 이제 gradient를 콜 해주기만 하면 전부 마지막에 연산됐던걸 활용해서 gradient를 뱉어주게 됩니다. 굉장히 편리하게 gradient를 계산할 수 있죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical에 비해서 bp는 엄청 빠른 속도로 동작됨을 볼 수 있습니다.\n",
    "## neuron을 64,128로 변경해보고 hidden_depth도 10,20으로 증가시켜 보면서 DNN을 구현해 보면 속도가 안정적으로 조금씩 증가하는것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
