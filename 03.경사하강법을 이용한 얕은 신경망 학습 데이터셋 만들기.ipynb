{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사 하강법을 이용한 얕은 신경망 학습  \n",
    "데이터 셋을 만들고, trainable parameter도 텐서플러우를 이용해 학습하는 과정을 진행해 보겠습니다.    \n",
    "텐서플러우 이용해서 얕은 신경망을 구현하고 경사하강법을 이용해 학습해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정  \n",
    "1000 epochs 까지 학습을 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의  \n",
    "이번엔 numpy가 아닌 tensorflow를 이용할 것입니다.\n",
    "### 얕은 신경망\n",
    "#### 입력 계층 : 2, 은닉 계층 : 128 (Sigmoid activation), 출력 계층 : 10 (Softmax activation)  \n",
    "-> 총 10개의 class 중에 하나로 들어갈 것입니다.  \n",
    "* 텐서플러우 2.0의 네트워크 구조하기 위해서는 케라스의 모델을 상속해서 클래스를 구현합니다.  \n",
    "* 상속을 했을 경우에는 상속을 한 initialize를 잊지 말고 해야합니다.  \n",
    "* 클래스의 모든 메서드들은 처음 파라메터를 self 로 받아주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.d1=tf.keras.layers.Dense(128,input_dim=2,activation='sigmoid')\n",
    "        self.d2=tf.keras.layers.Dense(10,activation='softmax')\n",
    "        \n",
    "    def call(self,x,training=None, mask=None):\n",
    "        x=self.d1(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "markdown", 
   "metadata": {},
   "source": [
    "## 학습 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model,inputs, labels, loss_object, optimizer, train_loss, train_metric):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction= model(inputs)\n",
    "        loss=loss_object(labels, predictions)\n",
    "    gradients=tape.gradient(loss,model.trainable_variables) #df(x)/dx\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_metric(labels,predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 생성, 전처리  \n",
    "랜덤시드를 고정해주면 여러번 시도해도 랜덤값은 항상 동일하게 된다.  \n",
    "센터포인트들이 중간에 둥글게 퍼지게 될겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "pts=list() #길이가 2\n",
    "labels=list() #10개의 class 중에 하나 0~9\n",
    "center_pts=np.random.uniform(-8.0,8.0,(10,2))\n",
    "for label, center_pt in enumerate(center_pts):\n",
    "    for _ in range(100):\n",
    "        pts.append(center_pt + np.random.randn(*center_pt.shape))\n",
    "        labels.append(label)\n",
    "pts=np.stack(pts,axis=0).astype(np.float32)\n",
    "labels=np.stack(labels, axis=0) \n",
    "train_ds=tf.data.Dataset.from_tensor_slices((pts,labels)).shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수 및 최적화 알고리즘 설정\n",
    "### CrossEntropy, Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ojbect=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 지표 설정\n",
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy=tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프  \n",
    "32개의 입력과 32개의 정답이 계속해서 나오게 됩니다.  \n",
    "매 에포크마다 데이터셋에서 입력이랑 정답받아서 트레인셋 돌려준다. 그리고 학습하는 과정을 출력해 준다. 그래서 Epoch, loss, Accuracy를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_object' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-ac0f83cdd5c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_object\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'Epoch: {}, Loss: {}, Accuracy: {}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_object' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for x, label in train_ds:\n",
    "        train_step(model,x,label,loss_object,optimizer,train_loss,train_accuracy)\n",
    "        \n",
    "    template= 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 및 학습 파라미터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('ch2_dataset.npz',inputs=pts,labels=labels)\n",
    "\n",
    "W_h,b_h=model.d1.get_weights()\n",
    "W_o,b_o=model.d2.get_weights()\n",
    "W_h =np.transpose(W_h)\n",
    "W_o=np.transpose(W_o)\n",
    "np.savez_compressed('ch2_parameters.npz',\n",
    "                   W_h=W_h,\n",
    "                   b_h=b_h,\n",
    "                   W_o=W_o,\n",
    "                   b_o=b_o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
