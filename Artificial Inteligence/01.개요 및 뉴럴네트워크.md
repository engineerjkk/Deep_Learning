### 가볍게 리뷰내용입니다. 내용이 엉성할 수 있습니다.

* 인공지능 : 기계 지능
1. 생명체 또는 인간이 갖고 있는 지능을 기계로 실현한 것
* 인공지능이 마음도 소유?
-> 앞으로의 연구

* AI의 발전

1. 앨런튜닝이 컴퓨터를 창시 : 1950
2. 1956 : 인공지능의 시작 
3. 1980 : AI의 암흑기
4. 1980s : 전문가 시스템(특정영역 : 법률과 의학분야)
5. 1990s : 암흑기
6. 2000 : 계속 AI에 관련된 연구 수행
7. 2006 : 딥러닝방식으로 해결가능해짐
8. 2010~ : 딥러닝

* AI의 기능 : 학습, 추론, 인지
1. 인공지능
2. 기계학습
3. 강화학습
4. 비지도학습
5. 지도학습
6. 딥러닝

* Neural Network 구조
1. 수사돌기 : 인접한 뉴런으로부터 신호가 세포체로 전달되는 통로
2. 세포체 : 신경세포의 중앙에위치 / 정보의 수용 연산 출력
3. 축색융기부 : 수상돌기를 통해 입력된 자극이 임계치를 넘으면 신경흥분을 전달, 낮으면 전달 X
4. 축색돌기 : 가늘고 긴 신경섬유로 인접한 뉴런으로 세포에 흥분을 전달
5. 시냅스 : 축색의 끝부분은 가느다란 가지로 나누어져 있고, 다른 뉴런의 수상돌기와 시냅스를 통해 연결
- 뉴런에서 뉴런으로 전달하는 부분
6. 뉴런의 정보교환 : 시냅스를 통해 수행

- 전기 신호에 의해 자극된 시냅스는 화학물질,신호를 방출한다.
- 흥분성 시냅스 : 시냅스 전 뉴런의 전기신호로 시냅스 후 전기신호의 전위를 올리는 경우
- 억제성 시냅스 : 시냅스 전 뉴런의 전기신호로 시냅스 후 전기신호의 전위를 내리는 경우

* 퍼셉트론 : 1943, 뇌의 뉴런 모델
- 다수의 인풋이 존재한다
- 가중치가 전달된다 : 시냅스와 관련된다.
- Suming junction : 수학적으로 표현가능하며 bias로 된다. 
- Activation function : 최종값을 결정하기 위함

* Summing junction에서 activationfunction이 적용되어 Output이 나오게된다. 

* XOR 또한 구현할 수 없는 모델이었다. 
-입력 두개와 weight, suming junction이 주어지고 값이 0.5보다 작으면 0 크면 1이라고 가정한다.
W0와 W1은 0.5보다 작아야하는데 W1과 W0자체는 0.5보다 커야한다. 따라서 간단한 XOR문제도 해결하지 못하는 문제가 있다. 그래서 이러한 문제를 해결하기 위해 2개 또는 3개의 Layer를 사용한다.
- 다층 퍼셉트론 : 3층 퍼셉트론으로 어떤 문제도 해결가능해진다

* Multi Later Perceptron
* MLP
- 입력층과 출력층 사이에 하나 이상의 은닉층을 가지는 전방향 신경회로망


# Artificial Inteligence 개요
1. Input layer 
2. Hidden Layer
3. Output Layer
입력에서 출력으로 향하는 방향 : Forward propagation

* sigmoid function
![image](https://user-images.githubusercontent.com/76835313/135761504-9456da77-4a21-42a9-a5c5-8e160579b769.png)

* z가 0이면 0.5가 되고 z가 커지면 1에수렴하고 음수가되면 무한대로 커져 0이된다.

* 학습 : Layer와 Layer를 연결하는 weight값을 우리가 원하는 output 값으로 만들어내나가는 과정

* Backpropagation 


* MSE(Mean Square Error)
- 양에러와 음에러
- 에러를 제곱을 취함으로써 모두 양의 값으로 바뀌어 소멸되는 오류를 방지할 수 있다. 
- 학습데이터를 이용하여 가중치 w와 바이어스 b를 변화시키는 과정을 반복적으로 수행하여 **cost function이 최소가 되도록 하는 것이 신경망 학습의 최종 목표**
- 뉴런과 입력의 개수가 많을 수록 복잡한 문제가 생긴다.
-> [BackPropagation](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
-> [BackPropagation](http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)  

* 경사하강법 : Gradient Descent Algorithm
![image](https://user-images.githubusercontent.com/76835313/135817714-75eb4be6-304b-4364-bfb8-0a2bbc8b9ef0.png)
- step을 통해서 Learning weight 값을 결정한다. 

* local 최저점에 빠지는 경우의 대책방안
- 이때는 스텝을 크게 가져가야한다.

* 인공지능 3대 요소기술
1. Learning 학습 : 사실과 규칙을 습득하는 일련의 과정
2. Inference 추론 : 주어진 사실이나 규칙으로부터 결론을 얻는 과정
3. Recognition 인식 : 분별하고 판단하는 과정

## 머신러닝
* 미셸 : 어떤 작업을 수행하는 머신의 성능을 P라고 했을 때, 경험 E를 통해 성능 P를 개선할 수 있다면 그 기계는 학습한다고 할 수 있다"라고 정의한다.

* 기계학습 시스템 : "주변환경 E에서 획득한 경험적 데이터 D를 기반으로 문제를 해결하는 모델 M을 자동으로 구성하고 또 스스로 성능 P를 향상하는 시스템"

