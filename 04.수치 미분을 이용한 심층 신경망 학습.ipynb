{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치 미분을 이용한 심층 신경망 학습  \n",
    "### 이번에는 역전파와 그렇지 않은 수치미분을 비교해 봅니다. 지금은 역전파가 아닌 수치 미분입니다.  \n",
    "### (비효율적인 프로그램)\n",
    "time을 이용해서 학습을 하는데 얼마나 시간이 걸리는지 측정해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유틸리티 함수  \n",
    "수치미분할때 입실론으로 나누고 입실론으로 더해서 로스펑션을 evaluate하는 부분이 들어가니 입실론을 정의해 줍니다.  \n",
    "매번 길게 쓰기는 힘드니 transpose는 _ t로 쓰겠습니다.  \n",
    "matmul도 _ m으로 쓰겠습니다. \n",
    "sigmoid 도 구현을 해 놓습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_error(h, y):\n",
    "    return 1 / 2 * np.mean(np.square(h - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴런 구현  \n",
    "w b a 를 받으면 저장합니다.  \n",
    "모델 파라메터들을 입력받으며, 각각의 그라디언트를 저장하기 위한 변수도 따로 만들어 줍니다.  \n",
    "dW라고 해서 d로 미분했다고 합시다.\n",
    "dw는 W와 똑같은 numpy object의 0으로 구성된 matrix가 생성됩니다.  \n",
    "bias도 마찬가지로 만들어 줍니다. 그러면 크기를 따로 입력하지 않아도 쉽게  만들 수 있습니다.  \n",
    "\n",
    "뉴런은 콜을했을때 activation function을 입력 받습니다. 그러면 W와 X를 곱할껀데 그전에 transpose를 넣어 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, W, b, a):\n",
    "        #Model Parameter\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.a=a\n",
    "        \n",
    "        #Gradients\n",
    "        self.dW=np.zeros_like(self.W)\n",
    "        self.db=np.zeros_like(self.b)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W),x)+self.b) #activation((W^T)x+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층신경망 구현\n",
    "init_var은 첫번째 출력이 random normal input output이다. 0이 평균이고 0.01이 standard variation입니다.  \n",
    "i by o의 메트릭스로 리턴하구요.   \n",
    "출력크기에 맞는 바이어스를 제로로 출력하는 메트릭스도 같이 리턴을 해줍니다.  \n",
    "그래서 w와 b를 initialize해줍니다.\n",
    "\n",
    "입력을 살펴보죠.  \n",
    "hidden_depth : 히든레이어가 몇개 들어갈 것인지.  \n",
    "num_neuron : 뉴런의 개수 (히든레이어 하나당)  \n",
    "input : input 레이어의 뉴런 개수\n",
    "output : output 레이어의 뉴런 개수\n",
    "activation finction은 기본적으로 sigmoid로 넣어줍니다.  \n",
    "relu라던지 다른 테스트삼아 구현해 넣어보시는 것도 좋습니다.  \n",
    "\n",
    "먼저 첫번째 히든레이어를 구현해 봅니다.  \n",
    "뉴런은 W,b,Activation을 입력으로 받는것을 기억합니다.  \n",
    "\n",
    "first가 아닌 리든레이어들은 여러개가 들어가니 for문을 사용해줍니다. 첫번째에 히든레이어를 만들어 줬으니 hidden_depth-1을 해줍니다. \n",
    "\n",
    "마지막은 activation을 softmax나 linear로 바꾸면 clssification등으로 구현할 수 있습니다. 지금은 그저 구현목적이니 이대로 냅두겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calc_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "\n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W,b=init_var(num_input,num_neuron)\n",
    "        self.sequence.append(Neuron(W,b,activation))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_depth -1):\n",
    "            W,b=init_var(num_neuron,num_neuron)\n",
    "            self.sequence.append(Neuron(W,b,activation))\n",
    "\n",
    "        # Output layer\n",
    "        W,b=init_var(num_neuron,num_output)\n",
    "        self.sequence.append(Neuron(W,b,activation))\n",
    "\n",
    "        #layer을 콜하는것은 뉴런을 콜하는 것입니다.\n",
    "        #그럼 한단계의 뉴런을 실행합니다. 위에 있는 리턴대로요\n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_gradient(self, x, y, loss_func):\n",
    "        def get_new_sequence(layer_index,new_neuron):\n",
    "            new_sequence=list()\n",
    "            for i, layer in enumerate(self.sequence):\n",
    "                if i == layer_index:\n",
    "                    new_sequence.append(new_neuron)\n",
    "                else:\n",
    "                    new_sequence.append(layer)\n",
    "            return new_sequence\n",
    "        \n",
    "        def eval_sequence(x,sequence):\n",
    "            for layer in sequence:\n",
    "                x=layer(x)\n",
    "            return x\n",
    "        \n",
    "        loss=loss_func(self(x),y)\n",
    "        \n",
    "        #self(x) = 추정한 것\n",
    "        #시퀀스를 돌면서 레이어를 뽑고, 웨이트에 하나하나 컬럼들이 나오게 된다.\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.sequence):\n",
    "            for w_i,w in enumerate(layer.W):\n",
    "                for w_j,ww in enumerate(w):\n",
    "                    W=np.copy(layer.W)\n",
    "                    W[w_i][w_j]=ww+epsilon\n",
    "                    new_neuron=Neuron(W,layer.b,layer.a)\n",
    "                    new_seq=get_new_sequence(layer_id,new_neuron)\n",
    "                    h=eval_sequence(x,new_seq)\n",
    "                    \n",
    "                    num_grad=(loss_func(h,y)-loss)/epsilon #(f(x+eps)-f(x))/epsilon\n",
    "                    layer.dW[w_i][w_j]=num_grad\n",
    "                for b_i,bb in enumerate(layer.b):\n",
    "                    b=np.copy(layer.b)\n",
    "                    b[b_i]=bb+epsilon\n",
    "                    new_neuron=Neuron(W,layer.b,layer.a)\n",
    "                    new_seq=get_new_sequence(layer_id,new_neuron)\n",
    "                    h=eval_sequence(x,new_seq)\n",
    "                    \n",
    "                    num_grad=(loss_func(h,y)-loss)/epsilon #(f(x+eps)-f(x))/epsilon\n",
    "                    layer.db[b_i]=num_grad\n",
    "            return loss # 매번 첫번째 loss를 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강 학습법  \n",
    "네트워크 입력, 학습데이터 입출력, 어떤로스쓸건지, 학습률을 0.01로 세팅  \n",
    " 입출력(데이터셋)과 로스받아서 그라디언트 계산해 loss로 받아준다.  \n",
    " 그라디언트가 계산된 레이어들을 순환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = network.calc_gradient(x, y, loss_obj)\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test loss 0.07886313075266332\n",
      "Epoch 1: Test loss 0.07886313075266332\n",
      "Epoch 2: Test loss 0.07886313075266332\n",
      "Epoch 3: Test loss 0.07886313075266332\n",
      "Epoch 4: Test loss 0.07886313075266332\n",
      "Epoch 5: Test loss 0.07886313075266332\n",
      "Epoch 6: Test loss 0.07886313075266332\n",
      "Epoch 7: Test loss 0.07886313075266332\n",
      "Epoch 8: Test loss 0.07886313075266332\n",
      "Epoch 9: Test loss 0.07886313075266332\n",
      "Epoch 10: Test loss 0.07886313075266332\n",
      "Epoch 11: Test loss 0.07886313075266332\n",
      "Epoch 12: Test loss 0.07886313075266332\n",
      "Epoch 13: Test loss 0.07886313075266332\n",
      "Epoch 14: Test loss 0.07886313075266332\n",
      "Epoch 15: Test loss 0.07886313075266332\n",
      "Epoch 16: Test loss 0.07886313075266332\n",
      "Epoch 17: Test loss 0.07886313075266332\n",
      "Epoch 18: Test loss 0.07886313075266332\n",
      "Epoch 19: Test loss 0.07886313075266332\n",
      "Epoch 20: Test loss 0.07886313075266332\n",
      "Epoch 21: Test loss 0.07886313075266332\n",
      "Epoch 22: Test loss 0.07886313075266332\n",
      "Epoch 23: Test loss 0.07886313075266332\n",
      "Epoch 24: Test loss 0.07886313075266332\n",
      "Epoch 25: Test loss 0.07886313075266332\n",
      "Epoch 26: Test loss 0.07886313075266332\n",
      "Epoch 27: Test loss 0.07886313075266332\n",
      "Epoch 28: Test loss 0.07886313075266332\n",
      "Epoch 29: Test loss 0.07886313075266332\n",
      "Epoch 30: Test loss 0.07886313075266332\n",
      "Epoch 31: Test loss 0.07886313075266332\n",
      "Epoch 32: Test loss 0.07886313075266332\n",
      "Epoch 33: Test loss 0.07886313075266332\n",
      "Epoch 34: Test loss 0.07886313075266332\n",
      "Epoch 35: Test loss 0.07886313075266332\n",
      "Epoch 36: Test loss 0.07886313075266332\n",
      "Epoch 37: Test loss 0.07886313075266332\n",
      "Epoch 38: Test loss 0.07886313075266332\n",
      "Epoch 39: Test loss 0.07886313075266332\n",
      "Epoch 40: Test loss 0.07886313075266332\n",
      "Epoch 41: Test loss 0.07886313075266332\n",
      "Epoch 42: Test loss 0.07886313075266332\n",
      "Epoch 43: Test loss 0.07886313075266332\n",
      "Epoch 44: Test loss 0.07886313075266332\n",
      "Epoch 45: Test loss 0.07886313075266332\n",
      "Epoch 46: Test loss 0.07886313075266332\n",
      "Epoch 47: Test loss 0.07886313075266332\n",
      "Epoch 48: Test loss 0.07886313075266332\n",
      "Epoch 49: Test loss 0.07886313075266332\n",
      "Epoch 50: Test loss 0.07886313075266332\n",
      "Epoch 51: Test loss 0.07886313075266332\n",
      "Epoch 52: Test loss 0.07886313075266332\n",
      "Epoch 53: Test loss 0.07886313075266332\n",
      "Epoch 54: Test loss 0.07886313075266332\n",
      "Epoch 55: Test loss 0.07886313075266332\n",
      "Epoch 56: Test loss 0.07886313075266332\n",
      "Epoch 57: Test loss 0.07886313075266332\n",
      "Epoch 58: Test loss 0.07886313075266332\n",
      "Epoch 59: Test loss 0.07886313075266332\n",
      "Epoch 60: Test loss 0.07886313075266332\n",
      "Epoch 61: Test loss 0.07886313075266332\n",
      "Epoch 62: Test loss 0.07886313075266332\n",
      "Epoch 63: Test loss 0.07886313075266332\n",
      "Epoch 64: Test loss 0.07886313075266332\n",
      "Epoch 65: Test loss 0.07886313075266332\n",
      "Epoch 66: Test loss 0.07886313075266332\n",
      "Epoch 67: Test loss 0.07886313075266332\n",
      "Epoch 68: Test loss 0.07886313075266332\n",
      "Epoch 69: Test loss 0.07886313075266332\n",
      "Epoch 70: Test loss 0.07886313075266332\n",
      "Epoch 71: Test loss 0.07886313075266332\n",
      "Epoch 72: Test loss 0.07886313075266332\n",
      "Epoch 73: Test loss 0.07886313075266332\n",
      "Epoch 74: Test loss 0.07886313075266332\n",
      "Epoch 75: Test loss 0.07886313075266332\n",
      "Epoch 76: Test loss 0.07886313075266332\n",
      "Epoch 77: Test loss 0.07886313075266332\n",
      "Epoch 78: Test loss 0.07886313075266332\n",
      "Epoch 79: Test loss 0.07886313075266332\n",
      "Epoch 80: Test loss 0.07886313075266332\n",
      "Epoch 81: Test loss 0.07886313075266332\n",
      "Epoch 82: Test loss 0.07886313075266332\n",
      "Epoch 83: Test loss 0.07886313075266332\n",
      "Epoch 84: Test loss 0.07886313075266332\n",
      "Epoch 85: Test loss 0.07886313075266332\n",
      "Epoch 86: Test loss 0.07886313075266332\n",
      "Epoch 87: Test loss 0.07886313075266332\n",
      "Epoch 88: Test loss 0.07886313075266332\n",
      "Epoch 89: Test loss 0.07886313075266332\n",
      "Epoch 90: Test loss 0.07886313075266332\n",
      "Epoch 91: Test loss 0.07886313075266332\n",
      "Epoch 92: Test loss 0.07886313075266332\n",
      "Epoch 93: Test loss 0.07886313075266332\n",
      "Epoch 94: Test loss 0.07886313075266332\n",
      "Epoch 95: Test loss 0.07886313075266332\n",
      "Epoch 96: Test loss 0.07886313075266332\n",
      "Epoch 97: Test loss 0.07886313075266332\n",
      "Epoch 98: Test loss 0.07886313075266332\n",
      "Epoch 99: Test loss 0.07886313075266332\n",
      "4.50150465965271 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
