{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "순환신경망 구현 및 학습.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LjScYlaXPnjt",
        "xYo6sposPuZg",
        "6scmGRe7PxaY",
        "dploJebFP1AI",
        "339bTj_DP4PA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjScYlaXPnjt"
      },
      "source": [
        "# 순환 신경망 구현 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQCOKsA5PakC"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYo6sposPuZg"
      },
      "source": [
        "#하이퍼 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_awhFffPbVK"
      },
      "source": [
        "EPOCHS = 10\r\n",
        "NUM_WORDS = 10000 #10000개의 단어만 사용하겠습니다."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6scmGRe7PxaY"
      },
      "source": [
        "#모델 정의  \r\n",
        "embedding : 특정단어들이 원 핫벡터에 하나씩 배정되는데, 특정 단어를 원한벡터로 하면 잘 안된다. 정수값이기 때문이다. 그래서 우리가 사용하는 실수로 가져오기위해 길이가 10000 원핫 벡터를 길이가 16인 feature vercor로 바꿔주는게 embedding의 역할입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXYHmN-VPcOM"
      },
      "source": [
        "class MyModel(tf.keras.Model):\r\n",
        "    def __init__(self):\r\n",
        "        super(MyModel, self).__init__()\r\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16)\r\n",
        "        self.rnn = tf.keras.layers.LSTM(32)\r\n",
        "        self.dense = tf.keras.layers.Dense(2, activation='softmax')\r\n",
        "        \r\n",
        "    def call(self, x, training=None, mask=None):\r\n",
        "        x = self.emb(x)\r\n",
        "        x = self.rnn(x)\r\n",
        "        return self.dense(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dploJebFP1AI"
      },
      "source": [
        "#학습, 테스트 루프 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O38xlGJFPdQC"
      },
      "source": [
        "# Implement training loop\r\n",
        "@tf.function\r\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        predictions = model(inputs, training=True)\r\n",
        "        loss = loss_object(labels, predictions)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "    train_loss(loss)\r\n",
        "    train_accuracy(labels, predictions)\r\n",
        "\r\n",
        "# Implement algorithm test\r\n",
        "@tf.function\r\n",
        "def test_step(model, images, labels, loss_object, test_loss, test_accuracy):\r\n",
        "    predictions = model(images, training=False)\r\n",
        "\r\n",
        "    t_loss = loss_object(labels, predictions)\r\n",
        "    test_loss(t_loss)\r\n",
        "    test_accuracy(labels, predictions)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339bTj_DP4PA"
      },
      "source": [
        "#데이터 셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OEWacc1PeAv",
        "outputId": "5654b505-6d8d-483a-c008-1994b03e2599"
      },
      "source": [
        "imdb = tf.keras.datasets.imdb\r\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\r\n",
        "\r\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\r\n",
        "                                                       value=0,\r\n",
        "                                                       padding='pre',\r\n",
        "                                                       maxlen=32)\r\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\r\n",
        "                                                      value=0,\r\n",
        "                                                      padding='pre',\r\n",
        "                                                      maxlen=32)\r\n",
        "\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\r\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK5SinIkP7Wx"
      },
      "source": [
        "#학습 환경 정의\r\n",
        "## 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5rTXzhrPe3F"
      },
      "source": [
        "# Create model\r\n",
        "model = MyModel()\r\n",
        "\r\n",
        "# Define loss and optimizer\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "# Define performance metrics\r\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n",
        "\r\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XSbB2ZlQHCB"
      },
      "source": [
        "#학습 루프 동작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmoSW6APPf_3",
        "outputId": "0e5a3a75-501f-4939-bc90-533795024545"
      },
      "source": [
        "for epoch in range(EPOCHS):\r\n",
        "    for seqs, labels in train_ds:\r\n",
        "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\r\n",
        "\r\n",
        "    for test_seqs, test_labels in test_ds:\r\n",
        "        test_step(model, test_seqs, test_labels, loss_object, test_loss, test_accuracy)\r\n",
        "\r\n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n",
        "    print(template.format(epoch + 1,\r\n",
        "                          train_loss.result(),\r\n",
        "                          train_accuracy.result() * 100,\r\n",
        "                          test_loss.result(),\r\n",
        "                          test_accuracy.result() * 100))\r\n",
        "    train_loss.reset_states()\r\n",
        "    train_accuracy.reset_states()\r\n",
        "    test_loss.reset_states()\r\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.5009668469429016, Accuracy: 74.66799926757812, Test Loss: 0.43555381894111633, Test Accuracy: 79.41600036621094\n",
            "Epoch 2, Loss: 0.3696795105934143, Accuracy: 83.51200103759766, Test Loss: 0.44655802845954895, Test Accuracy: 79.00399780273438\n",
            "Epoch 3, Loss: 0.31699296832084656, Accuracy: 86.29199981689453, Test Loss: 0.4646114408969879, Test Accuracy: 78.56800079345703\n",
            "Epoch 4, Loss: 0.2635522186756134, Accuracy: 88.96800231933594, Test Loss: 0.5097373127937317, Test Accuracy: 77.91600036621094\n",
            "Epoch 5, Loss: 0.20587608218193054, Accuracy: 91.63999938964844, Test Loss: 0.6313369870185852, Test Accuracy: 76.34400177001953\n",
            "Epoch 6, Loss: 0.16270530223846436, Accuracy: 93.46800231933594, Test Loss: 0.7402276396751404, Test Accuracy: 76.00399780273438\n",
            "Epoch 7, Loss: 0.12017648667097092, Accuracy: 95.43600463867188, Test Loss: 0.9673022627830505, Test Accuracy: 75.53600311279297\n",
            "Epoch 8, Loss: 0.09436867386102676, Accuracy: 96.4800033569336, Test Loss: 1.093193769454956, Test Accuracy: 75.01599884033203\n",
            "Epoch 9, Loss: 0.06927930563688278, Accuracy: 97.447998046875, Test Loss: 1.130053162574768, Test Accuracy: 74.47200012207031\n",
            "Epoch 10, Loss: 0.050641875714063644, Accuracy: 98.25999450683594, Test Loss: 1.3956888914108276, Test Accuracy: 74.93600463867188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUOV6aL3PiTl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}